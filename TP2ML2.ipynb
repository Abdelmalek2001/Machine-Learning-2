{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2161,
     "status": "ok",
     "timestamp": 1743044855483,
     "user": {
      "displayName": "Abdelmalek BELGA",
      "userId": "00718507867911878147"
     },
     "user_tz": 0
    },
    "id": "QgHlpk9Aka4l",
    "outputId": "dea0cd0f-b500-49de-d7f3-dbbd581c64b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2621,
     "status": "ok",
     "timestamp": 1743044880161,
     "user": {
      "displayName": "Abdelmalek BELGA",
      "userId": "00718507867911878147"
     },
     "user_tz": 0
    },
    "id": "6q0MpLwFkg8j",
    "outputId": "d65dabba-ccc8-4327-c235-c08cc3724c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[toy-text]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[toy-text]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[toy-text]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[toy-text])\n",
      "  Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[toy-text]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #FFD700; font-size: 26px; font-weight: bold;\">üéØ Objectif :</h2>\n",
    "<p style=\"font-size: 18px; color: #FFFFFF; margin-left: 20px;\">\n",
    "Mettre en pratique les concepts fondamentaux du Reinforcement Learning en explorant l‚Äôalgorithme Q-Learning.\n",
    "</p>\n",
    "\n",
    "<h2 style=\"color: #00FFFF; font-size: 26px; font-weight: bold;\">üîç Contenu :</h2>\n",
    "<ul style=\"font-size: 18px; color: #F0F8FF;\">\n",
    "  <li>Impl√©menter une <b>Q-table</b> et la mettre √† jour avec Q-Learning.</li>\n",
    "  <li>Comprendre l‚Äôimpact des strat√©gies d‚Äôexploration (<i>Œµ-greedy</i>).</li>\n",
    "  <li>Analyser la convergence des valeurs Q dans l‚Äôenvironnement <b>FrozenLake-v1</b> de OpenAI Gym.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #ADFF2F; font-size: 26px; font-weight: bold;\">üß© Exercices :</h2>\n",
    "\n",
    "<ul style=\"font-size: 18px; color: #00FFFF; margin-left: 20px;\">\n",
    "  <li style=\"margin-bottom: 10px;\">Exercice 1 : <span style=\"color: #FFD700; font-weight: bold;\">Exploration de l‚ÄôEnvironnement FrozenLake üßä</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30469,
     "status": "ok",
     "timestamp": 1743044912918,
     "user": {
      "displayName": "Abdelmalek BELGA",
      "userId": "00718507867911878147"
     },
     "user_tz": 0
    },
    "id": "7iG5ib2E6Cb_",
    "outputId": "4ab7a8a3-5216-4883-f2ef-26be6c631362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espaces d'actions : Discrete(4)\n",
      "Espaces d'observation: Discrete(16)\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 8, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 5, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 8, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 8, Reward : 0.0\n",
      "Action : 0, Observation : 12, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 8, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 3, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 3, Reward : 0.0\n",
      "Action : 0, Observation : 7, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 3, Reward : 0.0\n",
      "Action : 1, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "Action : 1, Observation : 3, Reward : 0.0\n",
      "Action : 0, Observation : 2, Reward : 0.0\n",
      "Action : 1, Observation : 6, Reward : 0.0\n",
      "Action : 3, Observation : 7, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 0, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, _ = env.reset()\n",
    "\n",
    "print(f\"Espaces d'actions : {env.action_space}\")\n",
    "print(f\"Espaces d'observation: {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")\n",
    "\n",
    "    if done:\n",
    "        observation, _ = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"font-size: 18px; color: #00FFFF; margin-left: 20px;\">\n",
    "  <li style=\"margin-bottom: 10px;\">Exercice 2 : <span style=\"color: #FFD700; font-weight: bold;\">Impl√©mentation de la Q-Table et Initialisation üìù</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29040,
     "status": "ok",
     "timestamp": 1743044949564,
     "user": {
      "displayName": "Abdelmalek BELGA",
      "userId": "00718507867911878147"
     },
     "user_tz": 0
    },
    "id": "oLGokuNN6Ff2",
    "outputId": "216ab77d-79c0-44a9-be1d-43ff695bbcf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espaces d'actions : Discrete(4)\n",
      "Espaces d'observation: Discrete(16)\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 5, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 8, Reward : 0.0\n",
      "Action : 0, Observation : 12, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 1, Observation : 3, Reward : 0.0\n",
      "Action : 0, Observation : 3, Reward : 0.0\n",
      "Action : 0, Observation : 2, Reward : 0.0\n",
      "Action : 3, Observation : 3, Reward : 0.0\n",
      "Action : 2, Observation : 3, Reward : 0.0\n",
      "Action : 0, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 6, Reward : 0.0\n",
      "Action : 3, Observation : 7, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 8, Reward : 0.0\n",
      "Action : 1, Observation : 8, Reward : 0.0\n",
      "Action : 1, Observation : 12, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 2, Observation : 4, Reward : 0.0\n",
      "Action : 0, Observation : 8, Reward : 0.0\n",
      "Action : 3, Observation : 4, Reward : 0.0\n",
      "Action : 2, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 3, Reward : 0.0\n",
      "Action : 2, Observation : 3, Reward : 0.0\n",
      "Action : 3, Observation : 3, Reward : 0.0\n",
      "Action : 1, Observation : 2, Reward : 0.0\n",
      "Action : 2, Observation : 2, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 1, Observation : 5, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 4, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Q-table initialization :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, _ = env.reset()\n",
    "\n",
    "print(f\"Espaces d'actions : {env.action_space}\")\n",
    "print(f\"Espaces d'observation: {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")\n",
    "\n",
    "    if done:\n",
    "        observation, _ = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "Q_table = np.zeros((16, 4))  # 16 √©tats, 4 actions\n",
    "print(\"Q-table initialization :\")\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"font-size: 18px; color: #00FFFF; margin-left: 20px;\">\n",
    "  <li style=\"margin-bottom: 10px;\">Exercice 3 : <span style=\"color: #FFD700; font-weight: bold;\">Algorithme Q-Learning et Mise √† Jour üîÑ</span></li>\n",
    "  <li style=\"margin-bottom: 10px;\">Exercice 4 : <span style=\"color: #FFD700; font-weight: bold;\">√âvaluation des Performances de l‚ÄôAgent üìä</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "executionInfo": {
     "elapsed": 1144560,
     "status": "error",
     "timestamp": 1743046276284,
     "user": {
      "displayName": "Abdelmalek BELGA",
      "userId": "00718507867911878147"
     },
     "user_tz": 0
    },
    "id": "5PEXqLd16c1t",
    "outputId": "f952fb75-d58a-4f50-8eb1-067a7544ff3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'actions : Discrete(4)\n",
      "Espace d'observations : Discrete(16)\n",
      "Q-table initiale :\n",
      "[[ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]\n",
      " [ 0.0  0.0  0.0  0.0]]\n",
      "√âpisode 500, Epsilon: 0.0816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ed602be5ca49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exploitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Mise √† jour de la Q-table (formule Q-Learning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"human\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;31m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prob\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/toy_text/frozen_lake.py\u001b[0m in \u001b[0;36m_render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"render_fps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             return np.transpose(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42) \n",
    "\n",
    "print(f\"Espace d'actions : {env.action_space}\")\n",
    "print(f\"Espace d'observations : {env.observation_space}\")\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "print(\"Q-table initiale :\")\n",
    "np.set_printoptions(formatter={'float': '{: .1f}'.format}, linewidth=75)\n",
    "print(Q_table)\n",
    "\n",
    "alpha = 0.1       \n",
    "gamma = 0.99     \n",
    "epsilon = 1.0    \n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "num_episodes = 5000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  \n",
    "        else:\n",
    "            action = np.argmax(Q_table[state, :]) \n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q_table[next_state, :]) - Q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    if (episode + 1) % 500 == 0:\n",
    "        print(f\"√âpisode {episode + 1}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "print(\"\\nQ-table apr√®s entra√Ænement :\")\n",
    "print(Q_table)\n",
    "\n",
    "num_test_episodes = 100\n",
    "success_count = 0\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated, truncated = False, False\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        action = np.argmax(Q_table[state, :])  \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        state = next_state\n",
    "\n",
    "    if terminated and reward == 1.0:  \n",
    "        success_count += 1\n",
    "\n",
    "success_rate = (success_count / num_test_episodes) * 100\n",
    "print(f\"\\nSucc√®s sur {num_test_episodes} √©pisodes : {success_count}\")\n",
    "print(f\"Taux de r√©ussite : {success_rate:.2f}%\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNTnyYJzD9hxgPvmliVjhtp",
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
